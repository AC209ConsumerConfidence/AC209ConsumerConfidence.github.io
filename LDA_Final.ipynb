{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "from ProgressBar import ProgressBar\n",
    "import scipy\n",
    "import pickle\n",
    "import cPickle\n",
    "\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Latent Dirichlet Allocation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><h3><a href=\"#Theory\">Theory</a></h3></li>\n",
    "<li><h3><a href=\"#Implementation\">Implementation</a></h3></li>\n",
    "<li><h3><a href=\"#Inspection\">Inspection</a></h3></li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output of the count vectorizer is too high dimensional (~10,000) to be particularly useful, especially because we find that no individual n-gram has a high signal-to-noise ratio. Therefore, we would like to perform some sort of dimensional reduction. One approach is to find the average sentiment of all words using the SentiWordNet dictionary. It's easy to argue that the sentiment of words in the business section reflects feelings about the economy. Another approach is topic modeling with Latent Dirichlet Allocation (LDA). While topics are not obviously correlated with CCI, it extracts interpretable information from the documents. That the topics generated from LDA are likely not very correlated to SentiWordNet scores makes it even more compelling to use both means of dimensional reduction.\n",
    "\n",
    "For LDA, we posit that the articles are generated by randomly pulling words from a mixture of topics and that the topics are defined by the unigrams and bigrams that they contain. After random initialization, the algorithm iterates through words and reassigns words to topics based on how often they occur in articles of that topic. Eventually, the allocation becomes more self-consistent as words move to topics where they are more common and articles become more concentrated under a few topics. As opposed to some clustering algorithms, the documents are assigned to a mixture of topics since the same words tend to occur across many articles so the articles are not fully separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual implementation of LDA is fairly complicated because there is a lot of iteration and sampling from distributions. However, it is already nicely implemented in sklearn. Still, it is not immediately clear how many topics one should use in LDA. To get an idea, we can inspect the topics that are generated. However, topics that we recognize may not be ideal for predicting CCI, so we also use cross validation. After cross validation, we found that 8 topics performs the best and is quite interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordMatrixBigrams = load_sparse_csr('bigramWordMatrix4.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics=8\n",
    "ldaBigrams = LatentDirichletAllocation(n_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldaDocsBigrams = ldaBigrams.fit_transform(wordMatrixBigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('ldaDocBigramScores4_8', ldaDocsBigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to see if the topics generated make some sense is to check the top 15 words in each topic. LDA gives the weighting of each component, so we can just argsort and match the indexes with the vocabulary used to generate the wordMatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"totalVocab4.pkl\", \"rb\") as input_file:\n",
    "    totalVocab = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_top_words = 15\n",
    "topic_words = []\n",
    "\n",
    "for topic in ldaBigrams.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([totalVocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stem 1</th>\n",
       "      <th>Stem 2</th>\n",
       "      <th>Stem 3</th>\n",
       "      <th>Stem 4</th>\n",
       "      <th>Stem 5</th>\n",
       "      <th>Stem 6</th>\n",
       "      <th>Stem 7</th>\n",
       "      <th>Stem 8</th>\n",
       "      <th>Stem 9</th>\n",
       "      <th>Stem 10</th>\n",
       "      <th>Stem 11</th>\n",
       "      <th>Stem 12</th>\n",
       "      <th>Stem 13</th>\n",
       "      <th>Stem 14</th>\n",
       "      <th>Stem 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>quarter</td>\n",
       "      <td>million</td>\n",
       "      <td>earn</td>\n",
       "      <td>share</td>\n",
       "      <td>report</td>\n",
       "      <td>compani</td>\n",
       "      <td>loss</td>\n",
       "      <td>cent</td>\n",
       "      <td>sale</td>\n",
       "      <td>revenu</td>\n",
       "      <td>net</td>\n",
       "      <td>year</td>\n",
       "      <td>incom</td>\n",
       "      <td>result</td>\n",
       "      <td>fiscal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>global</td>\n",
       "      <td>tax</td>\n",
       "      <td>oil</td>\n",
       "      <td>corp</td>\n",
       "      <td>energi</td>\n",
       "      <td>polici</td>\n",
       "      <td>follow</td>\n",
       "      <td>annual</td>\n",
       "      <td>gas</td>\n",
       "      <td>bankruptci</td>\n",
       "      <td>judg</td>\n",
       "      <td>20</td>\n",
       "      <td>million</td>\n",
       "      <td>file</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>percent</td>\n",
       "      <td>price</td>\n",
       "      <td>rate</td>\n",
       "      <td>stock</td>\n",
       "      <td>year</td>\n",
       "      <td>bank</td>\n",
       "      <td>month</td>\n",
       "      <td>rose</td>\n",
       "      <td>market</td>\n",
       "      <td>profit</td>\n",
       "      <td>said</td>\n",
       "      <td>fell</td>\n",
       "      <td>economi</td>\n",
       "      <td>increas</td>\n",
       "      <td>central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>said</td>\n",
       "      <td>compani</td>\n",
       "      <td>billion</td>\n",
       "      <td>bank</td>\n",
       "      <td>execut</td>\n",
       "      <td>plan</td>\n",
       "      <td>chief</td>\n",
       "      <td>busi</td>\n",
       "      <td>financi</td>\n",
       "      <td>deal</td>\n",
       "      <td>year</td>\n",
       "      <td>corpor</td>\n",
       "      <td>mr</td>\n",
       "      <td>new</td>\n",
       "      <td>chief execut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>million</td>\n",
       "      <td>new</td>\n",
       "      <td>york</td>\n",
       "      <td>new york</td>\n",
       "      <td>firm</td>\n",
       "      <td>agre</td>\n",
       "      <td>fund</td>\n",
       "      <td>group</td>\n",
       "      <td>stock</td>\n",
       "      <td>offer</td>\n",
       "      <td>exchang</td>\n",
       "      <td>secur</td>\n",
       "      <td>unit</td>\n",
       "      <td>insur</td>\n",
       "      <td>yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>share</td>\n",
       "      <td>onlin</td>\n",
       "      <td>health</td>\n",
       "      <td>compani</td>\n",
       "      <td>care</td>\n",
       "      <td>lead</td>\n",
       "      <td>18</td>\n",
       "      <td>canadian</td>\n",
       "      <td>pharmaceut</td>\n",
       "      <td>1989</td>\n",
       "      <td>medic</td>\n",
       "      <td>net inc</td>\n",
       "      <td>share earn</td>\n",
       "      <td>31</td>\n",
       "      <td>health care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>state</td>\n",
       "      <td>unit</td>\n",
       "      <td>market</td>\n",
       "      <td>said</td>\n",
       "      <td>unit state</td>\n",
       "      <td>expect</td>\n",
       "      <td>say</td>\n",
       "      <td>year</td>\n",
       "      <td>china</td>\n",
       "      <td>growth</td>\n",
       "      <td>thursday</td>\n",
       "      <td>street</td>\n",
       "      <td>debt</td>\n",
       "      <td>govern</td>\n",
       "      <td>sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>european</td>\n",
       "      <td>feder</td>\n",
       "      <td>euro</td>\n",
       "      <td>mani</td>\n",
       "      <td>trade</td>\n",
       "      <td>court</td>\n",
       "      <td>reserv</td>\n",
       "      <td>minist</td>\n",
       "      <td>rule</td>\n",
       "      <td>time</td>\n",
       "      <td>money</td>\n",
       "      <td>feder reserv</td>\n",
       "      <td>new</td>\n",
       "      <td>pound</td>\n",
       "      <td>union</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Stem 1   Stem 2   Stem 3    Stem 4      Stem 5   Stem 6  Stem 7  \\\n",
       "Topic 1   quarter  million     earn     share      report  compani    loss   \n",
       "Topic 2    global      tax      oil      corp      energi   polici  follow   \n",
       "Topic 3   percent    price     rate     stock        year     bank   month   \n",
       "Topic 4      said  compani  billion      bank      execut     plan   chief   \n",
       "Topic 5   million      new     york  new york        firm     agre    fund   \n",
       "Topic 6     share    onlin   health   compani        care     lead      18   \n",
       "Topic 7     state     unit   market      said  unit state   expect     say   \n",
       "Topic 8  european    feder     euro      mani       trade    court  reserv   \n",
       "\n",
       "           Stem 8      Stem 9     Stem 10   Stem 11       Stem 12     Stem 13  \\\n",
       "Topic 1      cent        sale      revenu       net          year       incom   \n",
       "Topic 2    annual         gas  bankruptci      judg            20     million   \n",
       "Topic 3      rose      market      profit      said          fell     economi   \n",
       "Topic 4      busi     financi        deal      year        corpor          mr   \n",
       "Topic 5     group       stock       offer   exchang         secur        unit   \n",
       "Topic 6  canadian  pharmaceut        1989     medic       net inc  share earn   \n",
       "Topic 7      year       china      growth  thursday        street        debt   \n",
       "Topic 8    minist        rule        time     money  feder reserv         new   \n",
       "\n",
       "         Stem 14       Stem 15  \n",
       "Topic 1   result        fiscal  \n",
       "Topic 2     file            11  \n",
       "Topic 3  increas       central  \n",
       "Topic 4      new  chief execut  \n",
       "Topic 5    insur     yesterday  \n",
       "Topic 6       31   health care  \n",
       "Topic 7   govern          sale  \n",
       "Topic 8    pound         union  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicDF = pd.DataFrame(topic_words)\n",
    "topicDF.index = ['Topic {}'.format(i) for i in range(1,(num_topics+1))]\n",
    "topicDF.columns = ['Stem {}'.format(i) for i in range(1,(num_top_words+1))]\n",
    "topicDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the topics seem to make some sense. For example, Topic 6 clearly has to do with \"healthcare\", \"pharmaceuticals\", and \"medicine\", but it is also interesting to note that \"Canada\" comes up a lot in those articles. This is not surprising at all as their system of socialized medicine is often referenced in the US. Also, Topic 8 has to do more with monetary policy as it includes \"Euro\", \"Pound\", \"money\", and \"Federal Reserve\". Something particularly interesting about Topic 8 is that \"bank\" does not appear. While I assume that \"Federal\", \"Reserve\", and \"Bank\" all appear frequently in that order, \"bank\" is more likely to occur elsewhere in the business section, not just regarding monetary policy. This indicates how LDA does not just group words that occur in articles together, it weights against including words that occur frequently across all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can group by month and save the topic scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('allDataWithStems.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = data.groupby('yearmonth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topicsByMonthBigrams = np.zeros((len(grouped.groups.keys()),ldaDocsBigrams.shape[1]))\n",
    "for i, month in enumerate(np.sort(grouped.groups.keys())):\n",
    "    topicsByMonthBigrams[i] = np.mean(ldaDocsBigrams[grouped.get_group(month).index], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('topicsByMonthBigrams4_8', topicsByMonthBigrams)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:AC209]",
   "language": "python",
   "name": "conda-env-AC209-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
