{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case you haven't installed the API\n",
    "! pip install nytimesarticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kshain/anaconda/envs/AC209/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "from ProgressBar import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr width=80%>\n",
    "# <center>Obtaining the Data</center>\n",
    "<hr width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### [Consumer Confidence Index](#Consumer-Confidence-Index)\n",
    "* ### [New York Times Articles](#New-York-Times-Articles)\n",
    "    * [Article Search API](#Article-Search-API)\n",
    "    * [Peculiarities of the API](#Peculiarities-of-the-API)\n",
    "    * [Downloading the Data](#Downloading-the-Data)\n",
    "    * [Working with the Files](#Working-with-the-Files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumer Confidence Index\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumer confidence index (CCI) is based on survey results of real consumers. They are asked their opinions of current and future economic conditions as well as about their personal economic situation. These survey questions are encoded and normalized to a baseline of 100 coming from the 1985 results. These results are obtained on a monthly basis by the Organisation for Economic Co-operation and Development and can be downloaded directly as a CSV from <https://data.oecd.org/leadind/consumer-confidence-index-cci.htm>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Times Articles\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Search API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York Times Article Search API allows for searching and obtaining headlines and lead paragraphs of articles dating back to 1851. Along with each article, there is metadata like the date it was published and the section in which it appeared. There is definitely some possibility that not all articles make it into the database, but an inspection of modern articles finds an order of 10s of articles per day which seems reasonable. The API call returns the data as JSON which can be used as such or transformed into CSV.\n",
    "\n",
    "To access the API, one needs to obtain an API key from <https://developer.nytimes.com/signup>. And install the API using:\n",
    "```python\n",
    "! pip install nytimesarticle\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('ca372b5c9318406780fe9ebef28e96a1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peculiarities of the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note are the usage limits for the API. Calls are limited to 1000 per day and 5 per second. Therefore, we need to make sure that our function sleeps between each call. The trickier issue with the API is that it will only return 100 pages of results from any given search. This means that searching for a year long window will have too many results and you will just get the first few weeks which fills the 100 pages. For this reason, we iterate through search windows of one week and monitor the number of pages found to make sure that it never exceeds 100 from any given search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save each year of data as a separate CSV. The steps of the downloading the data to CSV is as follows.\n",
    "1. Denote the first week long interval to search\n",
    "* Make an API call to search for articles in that week from the business section\n",
    "* Check how many pages are returned from the search\n",
    "* Iterate through the pages and articles in the page\n",
    "* Extract data from JSON and put into CSV format\n",
    "* After getting one week of data as a CSV, append to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadToFile(startdate, enddate, filename):\n",
    "    \"\"\"\n",
    "    Makes API calls to extract id, publication date, headline, and lead paragraph from NY Times articles in the date range.\n",
    "    Then, saves the data to a local file in csv format.\n",
    "    startdate: start of date range to extract (yyyymmdd)\n",
    "    enddate: end of date range to extract (yyyymmdd)\n",
    "    filename: csv file to create and append to\n",
    "    \"\"\"\n",
    "    \n",
    "    startdate = datetime.datetime.strptime(str(startdate), '%Y%m%d')\n",
    "    enddate = datetime.datetime.strptime(str(enddate), '%Y%m%d')\n",
    "\n",
    "    sliceStart = startdate\n",
    "\n",
    "    while (sliceStart<enddate):\n",
    "        leads = []\n",
    "        ids = []\n",
    "        dates = []\n",
    "        headlines = []\n",
    "        \n",
    "        sliceEnd = min(sliceStart + datetime.timedelta(weeks=1), enddate)\n",
    "\n",
    "        sliceStartInt = int(sliceStart.strftime('%Y%m%d'))\n",
    "        sliceEndInt = int(sliceEnd.strftime('%Y%m%d'))\n",
    "        print 'Downloading from {} to {}'.format(sliceStartInt, sliceEndInt)\n",
    "        while True:\n",
    "            try:\n",
    "                numhits = api.search(fl = ['_id'],begin_date = sliceStartInt, end_date=sliceEndInt,fq = {'section_name':'Business'}, page=1)['response']['meta']['hits']\n",
    "                time.sleep(1)\n",
    "                break\n",
    "            except:\n",
    "                print 'JSON error avoided'\n",
    "        pages = int(math.ceil(float(numhits)/10))\n",
    "        time.sleep(1)\n",
    "        pbar2 = ProgressBar(pages)\n",
    "        print '{} pages to download'.format(pages) # Note that you can't download past page number 100\n",
    "        for page in range(1,min(pages+1,100)):\n",
    "            while True:\n",
    "                try:\n",
    "                    articles = api.search(fl= ['_id','headline','lead_paragraph','pub_date'], begin_date = sliceStartInt, end_date=sliceEndInt,fq = {'section_name':'Business'}, page=page)\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "                except:\n",
    "                    print 'JSON error avoided'\n",
    "            \n",
    "            pbar2.increment()\n",
    "            for i in articles['response']['docs']:\n",
    "                if (i['lead_paragraph'] is not None) and (i['headline'] != []):\n",
    "                    headlines.append(i['headline']['main'])\n",
    "                    leads.append(i['lead_paragraph'])\n",
    "                    ids.append(i['_id'])\n",
    "                    dates.append(i['pub_date'])\n",
    "\n",
    "        pbar2.finish()\n",
    "        sliceStart = sliceEnd\n",
    "\n",
    "        zipped = zip(ids, dates, headlines, leads)\n",
    "        if zipped:\n",
    "            with open(filename, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                for line in zipped: \n",
    "                    writer.writerow([unicode(s).encode(\"utf-8\") for s in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from 19900101 to 19900108\n",
      "39 pages to download\n",
      "Complete! Total Elapsed time: 54.0 seconds                        \n",
      "Downloading from 19900108 to 19900115\n",
      "61 pages to download\n",
      "Complete! Total Elapsed time: 82.5 seconds                        \n"
     ]
    }
   ],
   "source": [
    "downloadToFile(19900101, 19900115, 'Sample_Output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's just check what we have in the files now. We can iterate over the yearly CSV files to make a dataframe with all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4fd1aa888eb7c8105d6c860e</td>\n",
       "      <td>1990-01-03T00:00:00Z</td>\n",
       "      <td>Tandem Expected To Show Computer</td>\n",
       "      <td>LEAD: Tandem Computers Inc. is expected to int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52b85b7738f0d8094087c782</td>\n",
       "      <td>1990-01-03T00:00:00Z</td>\n",
       "      <td>Chrysler Shows Van Concept</td>\n",
       "      <td>LEAD: The Chrysler Corporation today introduce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52b85b7638f0d8094087c780</td>\n",
       "      <td>1990-01-02T00:00:00Z</td>\n",
       "      <td>Loan Pact Seen For Hungary</td>\n",
       "      <td>LEAD: Hungary expects to complete a deal with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b85b7538f0d8094087c77f</td>\n",
       "      <td>1990-01-02T00:00:00Z</td>\n",
       "      <td>Counterattack Planned By Lawyers for Lincoln</td>\n",
       "      <td>LEAD: Lawyers for Charles H. Keating Jr., who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fd18d4c8eb7c8105d691815</td>\n",
       "      <td>1990-01-08T00:00:00Z</td>\n",
       "      <td>Intermetrics Inc reports earnings for Qtr to N...</td>\n",
       "      <td>LEAD: *3*** COMPANY REPORTS ** *3* Intermetric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                  date  \\\n",
       "0  4fd1aa888eb7c8105d6c860e  1990-01-03T00:00:00Z   \n",
       "1  52b85b7738f0d8094087c782  1990-01-03T00:00:00Z   \n",
       "2  52b85b7638f0d8094087c780  1990-01-02T00:00:00Z   \n",
       "3  52b85b7538f0d8094087c77f  1990-01-02T00:00:00Z   \n",
       "4  4fd18d4c8eb7c8105d691815  1990-01-08T00:00:00Z   \n",
       "\n",
       "                                            headline  \\\n",
       "0                   Tandem Expected To Show Computer   \n",
       "1                         Chrysler Shows Van Concept   \n",
       "2                         Loan Pact Seen For Hungary   \n",
       "3       Counterattack Planned By Lawyers for Lincoln   \n",
       "4  Intermetrics Inc reports earnings for Qtr to N...   \n",
       "\n",
       "                                                lead  \n",
       "0  LEAD: Tandem Computers Inc. is expected to int...  \n",
       "1  LEAD: The Chrysler Corporation today introduce...  \n",
       "2  LEAD: Hungary expects to complete a deal with ...  \n",
       "3  LEAD: Lawyers for Charles H. Keating Jr., who ...  \n",
       "4  LEAD: *3*** COMPANY REPORTS ** *3* Intermetric...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_list = []\n",
    "for year in range(1990,1992):\n",
    "    data = pd.read_csv('{}_Output.csv'.format(year), header=None)\n",
    "    all_data_list.append(data) # list of dataframes\n",
    "data = pd.concat(all_data_list, axis=0)\n",
    "data.columns = ['id','date','headline', 'lead']\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python [conda env:AC209]",
   "language": "python",
   "name": "conda-env-AC209-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
